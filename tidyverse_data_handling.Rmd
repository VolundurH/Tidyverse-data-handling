---
title: "Data handling using the Tidyverse"
author: "Völundur Hafstað, Deborah Figueiredo Nacer de Oliveira"
output:
  pdf_document:
    toc: yes
    toc_depth: 3
    fig_width: 4
    fig_height: 2.5
    df_print: tibble
  html_document:
    theme: united
    toc: yes
urlcolor: blue

---
```{r, include=FALSE}
options(tinytex.verbose = TRUE)
```


# Introduction

In this section of the course we will be exploring many of the core functions of the tidyverse. The tidyverse is a collection of packages that are designed for data science. They are designed to work very well with each other and together form an ecosystem that is ideal for analyzing most forms of data. Each of the functions we will cover is relatively simple, but when used together are capable of very fast, efficient and readable data analysis. We will be working with sample data sets that we have provided in the course material. 

This file is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. It is a very convenient file format when you are writing lots of text alongside your R code, for example when writing reports. In an R markdown file you can insert code "chunks" and run them individually. They will appear in the document as gray text boxes. The output of that code is then output directly underneath the code chunk.

We start by loading in the tidyverse suite of packages using the `library()` function:

```{r}
library(tidyverse)
```

Libraries are generally loaded at the very beginning of the file, so that you and other people can see at a glance what packages you are using. In this section of the course we will only load the tidyverse library, which is a collection of several packages in one. 

# Reading in data

We have provided several files containing data in the course material. Make sure to set your working directory to the folder you have downloaded these files to. In my case they are located in a folder called "r_course", in a sub-folder called "data".

```{r}
setwd("/home/v/projects/r_course/")
```

Now R knows where to look for any files that you want to read in. To actually open them in R we will use `read_tsv()`. Since we have already set a working directory, we do not need to give R the full path to the file, just the relative path from the working directory.

The first data set we will work with is a list of all domestic flights departing from three New York airports in 2013. When we read in data, we want to assign it to a variable name that makes it obvious what we are working with. In this case a simple name like "flights" will do nicely.

```{r}
flights <- read_tsv("data/nycflights13_flights.txt")
```

When we read in data with `read_tsv()`, it gives some useful information about the file we have just read in, although this can be disabled. Here we can see that the file we read in has around 336,000 rows of data and 19 columns. `read_tsv()` also tries to guess the type of each column, i.e. whether it is a character, numeric, logical etc.

To get a better idea of what the data looks like we can use the functions `head()` and `tail()` to see the first and last rows of the data.

```{r}
head(flights)
tail(flights)
```

Another great function to quickly see what you are working with is `glimpse()`.

```{r}
glimpse(flights)
```

`glimpse()` shows you every column in the data frame, which can be convenient if you are working with many columns of data.

# Data wrangling

One of the most important aspects of working with large data sets is to be able to easily find the exact data that you need.

For example, let's say that we are only interested in how far the average flight from NYC is. The flights data frame contains much more information than that, but the only column of data we are interested in is "distance".

We might also only be interested in flights departing from JFK airport. The flights data contains information from three different airports in an approximately even ratio, so in this case about two-thirds of the rows are not relevant to us.

This section will show you three simple functions that you can use to extract just the data that you need.

## filter()

`filter()` is a function that subsets rows, keeping only those that we are interested in. We can tell the function what rows we are interested in by giving it a "condition" to filter by. For example to filter flights that originate from JFK we would use:

```{r}
filter(flights, origin == 'JFK')
```

Here we supplied the `filter()` function with two things. The first is the data frame "flights". This tells `filter()` on what data frame we want to apply the filter. The next is the filter condition: `origin == 'JFK'`. "origin" is the name of the column in the data frame that we are using to filter the data, and 'JFK' is what needs to be contained in that particular cell in order to be kept. Note that when working with strings you need to wrap it inside single or double quotes.´ The double equal sign "==" is a comparison operator and will return either true or false. Note that this is **not** the same as "=", which can be used to assign variables. One of the most common sources of errors when you are beginning to learn R is to write "=" when you meant to write "=="!. For more information on conditionals see the R_building_blocks.pdf file.

The `filter()` function can be supplied with more than one conditional. For example if we want to find flights between JFK and Miami airport (MIA) that departed between 6 and 8 in the morning, we can string all of those together using the "&" (and) and "|" (or) symbols.

```{r}
filter(flights, origin == "JFK" & dest == "MIA" & dep_time > 600 & dep_time < 800)
```

### Exercise 1

1.  How many flights departed JFK in February?
2.  How many flights departed LGA airport in June and July combined?

## select()

The `select()` function is similar to `filter()`, but it subsets columns instead of rows.

```{r}
select(flights, tailnum, air_time, distance)
```

The first argument we supply to `select()` is the name of the data frame, all subsequent arguments are the names of the columns we want to keep. The order of the columns in the output will be the order you supply them to the function.

`select()` is quite flexible in how you specify the column names. You can give it the column number and/or the column name, and the column names do not have to be in quotes:

```{r}
select(flights, 1:3, 7, origin, "dep_delay", arr_delay)
```

Note that this is quite ugly code, especially the mix of quoted and unquoted column names!

You can also select columns that are in a character vector:

```{r}
a <- c("origin", "dest")
select(flights, a)
```

Be careful with this as you will get an error if there are strings in the vector that are not column names in the data frame.

## mutate()

Often what we are interested in is not explicitly given in our data but can be calculated using values from several columns. We can create new columns in our data frame using the `mutate()` function. For example, let's say that we are interested in how fast an airplane was going. We can calculate the speed using information from the "distance" and "air_time" columns and store it in a new column "speed":

```{r}
mutate(flights, speed = distance / air_time)
```

The resulting data frame is now 20 columns instead of 19, with the new column "speed" positioned last.

# Code styles and the pipe

The true power of the tidyverse functions comes when we string them together to perform complex analyses using many relatively simple steps.

There are many ways to write code. Everyone has their own preferred way to write and structure code in a way that makes sense to them. It is worth keeping in mind that your code should be readable by other people, and when you start working on complicated projects with other people this becomes absolutely essential.

The "programmer" way - if you have used python or a similar programming language this might feel natural:

```{r echo=TRUE,results='hide'}
x <- filter(flights, origin == "JFK")
x <- select(x, tailnum, air_time, distance)
x <- mutate(x, speed = distance / air_time)
x
```

Here we assigned the new data frame and its intermediate steps into the variable "x". In each line of code we overwrite x by applying a new function to it, modifying it in some way. This is a good way to write your code as it makes it fairly readable for other people. A good rule of thumb is to only do "one thing" in each line of code (in this case `filter()`, `select()`, `mutate()`). The downside of this style is it does get a bit redundant always having to type `x <- ...`.

R doesn't care if your code is readable or not as long as it executes without errors. If you wanted to you could get exactly the same output using a single line of code using the "unreadable" way:

```{r echo=TRUE,results='hide'}
y <- 
  mutate(select(filter(flights, origin == "JFK"), tailnum, air_time, distance), 
    speed = distance / air_time) # complicated and unreadable code! 
y
```

To R this is exactly the same block of code. In fact it is probably a bit more efficient since it does not have to store the intermediate steps! However this is completely unreadable for other people and you should avoid writing code like this as much as you can. In cases where you are forced to write out code like this it is important to comment the code so that people know at a glance what it is you have done.

We will be using a bit of a different way to write our code, using something known as the [pipe](https://magrittr.tidyverse.org/reference/pipe.html). The pipe is written `%>%` (keyboard shortcut ctrl+shift+m or command+shift+m) and is read as *and then*. It replaces the first argument of a function on its right with whatever output comes from the code on the left. For example

```{r echo=TRUE,results='hide'}
flights %>%                # take the "flights" data frame, and then
  filter(origin == "JFK")  # apply the filter() function
```

is the same thing as writing:

```{r echo=TRUE,results='hide'}
filter(flights, origin == "JFK")
```

You would read the code as taking the flights data *and then* applying the `filter()` function. Notice that when we use the pipe we do not have to type in "flights" inside `filter()` anymore, only the filter conditions. This makes it very easy to read other people's code, as it is executed in the same order as we read it.  

The beauty of the pipe is that we can string together multiple functions like this, to make a chain of pipes that is as long as we need. So the "tidyverse" way using pipes looks like:

```{r}
z <- flights %>%                                             # take the flight data, and then
  filter(origin == "JFK") %>%                                # filter that data, and then
  select(year, month, day, tailnum, air_time, distance) %>%  # select the columns we need, and then
  mutate(speed = distance / air_time)                        # mutate to create a new column
z
```

In the end this looks a lot like the "programmer" style of writing code, but without the redundancy of assigning the intermediate steps or specifying what data frame it is that we want to use to each function.

### Exercise 2

Look at the data set "diamonds" that comes built-in to the ggplot2 package (part of the tidyverse suite):

1.  Find the diamond with the highest price/volume ratio - how many carats is it? Try to write your code using pipes! Hint: use the arrange() function to sort the data. Check out the documentation by typing `?arrange()`.

```{r}
diamonds
```

# Summarising data

In this section we will cover a couple of functions that are great to use to generate summary statistics - these are often the first steps when it comes to analyzing any type of numerical data.

One of the simplest ways of summarising data is by counting the number of observations based on some criteria. Let's say we want to find out how many flights have departed from New York each month. There are several ways to do this:

```{r}
flights %>% 
  group_by(month) %>% 
  summarise(n = n())

flights %>% 
  group_by(month) %>% 
  tally()

flights %>% 
  count(month)
```

Personally I like to use either the first or last options here. The `summarise()` function is very powerful and can generate more summary statistics for you, such as mean, median, min, max and standard deviation. For an overview of what summary functions can be used inside `summarise()` please see the [dplyr cheat sheet](https://btep.ccr.cancer.gov/docs/rintro/resources/dplyr_cheatsheet.pdf).

Let's make a table that shows the number of outgoing flights with some other summary statistics:

```{r}
flights %>% 
  group_by(month) %>% 
  summarise(n = n(), 
            mean_dep_delay = mean(dep_delay), 
            min_dep_delay = min(dep_delay), 
            max_dep_delay = max(dep_delay))
```

There is no limit to how many summary statistics you can generate inside a single `summarise()`, as long as they are separated by a comma. Here I have also split each summary into a separate line in the code to improve readability - a good practice!

Note however that there is something strange going on with the new columns - their values are all NA. This means that there is at least one "dep_delay" observation in the original data that has an NA value. We need to remove those first before generating summary statistics. We can do this by either outright removing rows that contain an NA, or converting the NAs to another value such as 0. In this case we will just remove them. We do this with the `drop_na()` function. Many of these summary functions such as `mean()` also accept an optional field "na.rm" which removes the NAs only for that particular calculation (shown in the code).

```{r}
flights %>% 
  drop_na(dep_delay) %>%  # remove rows that contain an NA in the dep_delay column
  group_by(month) %>% 
  summarise(n = n(), 
            mean_dep_delay = mean(dep_delay, na.rm = T), # can also use na.rm = T instead of drop_na()
            min_dep_delay = min(dep_delay), 
            max_dep_delay = max(dep_delay))
```

If you don't supply any arguments to `drop_na()` it will look at the entire row of data and remove it if it sees an NA in any column. Alternatively you can supply it with column names like we have done here.

### Exercise 3

1.  What is the average flight distance from each airport?
2.  Which carrier has the worst departure delays on average?
3.  (Advanced) Can you separate the effects of bad airports and bad carriers?

# Joins

The concept of joins can be a bit tricky to grasp, but it is very helpful to know how to use them. Often if we are working with complicated data we store information in separate files. For example, observations related to some disease and patient metadata are often stored in separate files. We can then link the information from the two files using one or more "key" columns present in both files, such as a patient ID.

[This image](https://statisticsglobe.com/wp-content/uploads/2019/04/dplyr-data-join-functions-overview_inner_join-left_join-right_join-full_join-semi_join-anti_join.png) shows the dplyr joins and how they work.

Let's say we want to find out how many seats on average an airplane has for each of the three NYC airports. To achieve this we need to read in a second data set that has information on the planes:

```{r echo=TRUE,results='hide'}
planes <- read_tsv("data/nycflights13_planes.txt")
```

Take a quick look at the data:

```{r}
glimpse(planes)
```

The information that links this and the flights data is the column "tailnum". Let's connect the "origin" column in the flights data to all the data in the planes data, using the "tailnum" column that appears in both data sets.

```{r}
flights %>% 
  select(origin, tailnum) %>% 
  left_join(planes, by = "tailnum")
```

Here we used a so-called "left join". Notice that the resulting data frame has the same number of rows as the original flights data frame. This is probably the most common type of join when using pipes, as we often have a "main" data frame (in this case it is flights) that we want to add some additional info columns to that are stored elsewhere (in this case - planes) without deleting or duplicating information in the main data.

There are some pitfalls when it comes to joining tables. If the key column contains duplicated values, e.g. if the "tailnum" column in the planes data had two rows with an identical tail number, those rows would be duplicated when using joins. This will mess up any downstream summary statistics you would perform on the data. See the example below.

### An example of how joins can duplicate rows and skew summary statistics

Let's create two data frames to join. They will be similar to the flights data but much smaller. The first one will be tree rows, two columns: flights 1-3 with tail numbers A1-A3. 

```{r}
a <- tibble(flight = c(1,2,3), tailnum = c("A1","A2","A3"))
```

The second will be information on when the planes were built. Notice that someone made a mistake when creating this data, and has accidentally copied the A1 information twice! 

```{r}
b <- tibble(tailnum = c("A1", "A1", "A3", "A4"), year = c(2000, 2000, 2020, 2022))
```

Now look what happens when we join these tables:

```{r}
c <- a %>% 
  left_join(b, by = "tailnum")
c
```

Since we have no information on tail number A2 in the b data frame, that value is listed as NA in the joined table. Also notice that there is no row containing A4 because we used a left join. If we wanted to calculate the average age of the planes in our data, and we had not noticed this mistake, we would get a wrong value out of it:

```{r}
c %>% 
  drop_na() %>% 
  mutate(age = 2022 - year) %>% 
  summarise(mean_age = mean(age))
```

In this example it is obvious what is happening, but when working with large data sets with millions of observations, this can give you a real headache!

### Exercise 4

1.  Try joining together the dataframes `a` and `b` like in the example above but replacing `left_join()` with `right_join()`, `inner_join()`, `full_join()` and `anti_join()`. Can you predict what the resulting outputs will look like? 
2.  How would you join two tables when the key column has different names in each of the data frames?
3.  What are the full names of the three NYC airports? Use the "faa" column in nycflights13_airports.txt and joins to find out!
4.  What is the most common destination airport for each of the three NYC airports?
5.  Which carrier is the only one that flies to Ted Stevens Anchorage Intl? Make sure to show the full name found in the file nycflights13_airlines.txt!

# Plotting

We will dedicate a full day for plotting later in the week, but it is very convenient to summarise data visually, so in this section we will quickly introduce the basics of plotting.

The R package ggplot2 is considered to be one of the most powerful and versatile data-plotting tools out there. It can be quite complicated and the syntax will seem strange at first. However it works brilliantly with the pipe operator and the other packages of the tidyverse. For it to work properly the data needs to be in the correct format (long-format data, we will cover this later).

## Basic plots made with ggplot2

How many flights departed NYC each month?

```{r}
flights %>% 
  ggplot(aes(x = month)) +  # specify which data column goes on which axis
  geom_bar()                # make a barplot
```

What is the mean arrival delay for each airport?

```{r}
flights %>% 
  drop_na(arr_delay) %>% 
  ggplot(aes(x = origin, y = arr_delay)) +
  geom_boxplot()
```

At what time of day do flights generally depart?

```{r}
flights %>% 
  ggplot(aes(x = hour)) + 
  geom_histogram(bins = 23)
```

Can we spot a relation between departure delay and arrival delay? Note that here we are making a dot plot with one dot for each row of data - over 300,000 dots. Depending on the speed of your computer it might take a while to plot. If you feel like it takes too long to render you can filter random rows using the handy `slice_sample()` function.

```{r, }
flights %>% 
  slice_sample(n = 10000) %>% 
  ggplot(aes(x = dep_delay, y = arr_delay, col = origin)) + 
  geom_point()
```

### Exercise 5

1.  Make a boxplot that compares the three NYC airports based on distance flown.
2.  (Bonus) If you are feeling adventurous, plot the longitude and latitude of every airport in the airports data with `coord_quickmap()`. Visually filter out airports that are not on mainland USA.

# Pivoting

Take a look at the following tables:

```{r}
table1
table2
table3
table4a 
table4b
```

They all store the same information, but in different ways. To work effectively with data you need to be able to transform it into a format that is efficient to work with. What format is best to use depends on what data you have, your research question, and what tools you are using to answer that question.

The tidyverse packages are made to work with "tidy" data - hence the name tidyverse. There are three criteria which make a data set tidy:

1.  Each variable must have its own column.
2.  Each observation must have its own row.
3.  Each value must have its own cell.

### Exercise 6

1.  Take another look at the table1 - 4 data frames. Which one of them do you think meets all the requirements to be "tidy"?

Let's have a look at another data set:

```{r}
world_bank_pop %>% 
  head()
```

This data frame contains information about population numbers and growth for some countries over a span of several years. Is this data tidy?

If we want to take a look at how the population of a country changes over the years then this format is not optimal. ggplot2 and the other tidyverse packages work best when the data is in "long" format. In this case it would mean changing the data frame so that we replace columns 2000-2017 with a single column "year". This is done with the `pivot_longer()` function. Let's say we are only interested in the population growth for Sweden and Denmark:

```{r}
swe_den_population <- world_bank_pop %>% 
  filter(indicator == "SP.POP.TOTL") %>%      # filter to keep only total population information
  pivot_longer(cols = -c(country, indicator), # pivot all columns in the data except country and indicator 
               names_to = "year",             # specify the name of the key column
               values_to = "population") %>%  # specify the name of the value column
  filter(country %in% c("SWE", "DNK")) %>%    # filter to keep only Sweden and Denmark
  mutate(year = as.numeric(year))             # tell R that the year column contains numbers, not strings

swe_den_population
  
```

Now the data is in a form that is convenient to plot:

```{r}
swe_den_population %>% 
  ggplot(aes(x = year, y = population, col = country)) +  # plot the data, color by country
  geom_line() + # specify that we want a line plot
  theme_bw() +  # make it a bit more pretty to look at
  labs(title = "Population of Sweden and Denmark") # add a title for the plot
```


### Exercise 7

1.  Try using the `pivot_wider()` function to transform table2 so that it looks exactly like table1. Look at the documentation for `pivot_wider()` to see the correct syntax. Documentation can be accessed by adding a "?" to the beginning of a function:

```{r}
?pivot_wider()
```

Note: `pivot_wider()` has many optional arguments, the only ones you need to use for this exercise are "names_from" and "values_from".

# Working with strings

Complicated data often comes with string columns such as identifiers, names, and categorical values. Tidyverse comes with powerful tools to work with strings, most are found in the aptly named [stringr](https://edrub.in/CheatSheets/cheatSheetStringr.pdf) package. We will not focus so much on strings for now, but a couple of useful string-related functions are highlighted below.

For this section we will be using data on artwork found in the Tate Art Museum, artwork.csv. Note that this file, unlike the others we have worked with, is comma separated. To read in this data we use the `read_csv()` function.

```{r}
artwork <- read_csv("data/artwork.csv")
artwork %>% 
  glimpse()
```

Use `str_sub()` to extract substrings (notice that this is done inside a `mutate()`):

```{r}
artwork %>% 
  mutate(title_short = str_sub(title, 1, 10)) %>% # new column that contains characters 1-10 of title
  select(year, artist, title_short) 
  
```

Use `str_replace()` to replace parts of strings:

```{r}
artwork %>% 
  select(artist) %>% 
  mutate(artist = str_replace(artist, "Blake", "John")) %>% # Replaces the first occurrence of "Blake" with "John"
  head()
  
```

`separate()` splits the column into two or more new columns. `paste0()` can merge columns together (although that is not the only thing it can be used for!):

```{r}
artwork %>% 
  select(artist) %>% 
  head() %>% 
  separate(artist, into = c("last_name", "first_name"), sep = ",", remove = F) %>% # keep original column
  mutate(full_name = paste0(first_name, " ", last_name))
```

The stringr functions also support regular expression (shortened as regex), a way of searching for specific patterns in strings without knowing the exact sequence of characters. For example if you are looking for a subset of patient identifiers that all start with the letter A, followed by 4 digits and ending with the letter S, you would use regular expression to find them. We will not cover regex in this course, but the stringr cheat sheet has an excellent overview of it. Regex is not only used in data science but in many fields of programming, and being able to work with it is a very valuable skill to have.

# Other useful functions we have not covered

`distinct()` - gives all unique values in a column 

`pull()` - extracts a column into a 2D vector 

`arrange()` - orders the data based on a given column, either alphabetically or numerically 

`rename()` - renames a column

`row_number()` - retrieves row number 

`slice()` - subsets rows by row number

`slice_max()` - subsets row(s) with the maximum value of a given column

`as_tibble()` - converts data into a tibble (the tidyverse version of a data frame)
